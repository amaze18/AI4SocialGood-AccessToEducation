{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIOLQiiRLijF"
      },
      "outputs": [],
      "source": [
        "! pip install llama-index\n",
        "! pip install span-marker\n",
        "! pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "m_Fnv1E4Ng7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "openai.api_key=os.environ['SECRET_TOKEN']\n",
        "from llama_index import SimpleDirectoryReader\n",
        "from llama_index.extractors.metadata_extractors import EntityExtractor\n",
        "from llama_index.node_parser import SentenceSplitter\n",
        "from llama_index.ingestion import IngestionPipeline\n",
        "from llama_index import ServiceContext, VectorStoreIndex\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index import (StorageContext,load_index_from_storage)\n",
        "from llama_index.memory import ChatMemoryBuffer\n",
        "from llama_index.embeddings import OpenAIEmbedding\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
        "def indexgenerator(indexPath, documentsPath):\n",
        "\n",
        "    # check if storage already exists\n",
        "    if not os.path.exists(indexPath):\n",
        "        print(\"Not existing\")\n",
        "        # load the documents and create the index\n",
        "\n",
        "        entity_extractor = EntityExtractor(prediction_threshold=0.2,label_entities=False, device=\"cpu\") # set device to \"cuda\" if gpu exists\n",
        "\n",
        "        node_parser = SentenceSplitter(chunk_overlap=102,chunk_size=1024)\n",
        "\n",
        "        transformations = [node_parser, entity_extractor]\n",
        "\n",
        "        documents = SimpleDirectoryReader(input_files=[r\"C:\\Users\\Kush Juvekar\\Desktop\\large_pdf\"]).load_data()\n",
        "\n",
        "        pipeline = IngestionPipeline(transformations=transformations)\n",
        "\n",
        "        nodes = pipeline.run(documents=documents)\n",
        "\n",
        "        service_context = ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0),embed_model=embed_model)\n",
        "\n",
        "        index = VectorStoreIndex(nodes, service_context=service_context)\n",
        "\n",
        "        # store it for later\n",
        "        index.storage_context.persist(indexPath)\n",
        "    else:\n",
        "        #load existing index\n",
        "        print(\"Existing\")\n",
        "        storage_context = StorageContext.from_defaults(persist_dir=indexPath)\n",
        "        index = load_index_from_storage(storage_context,service_context = ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0),embed_model=embed_model))\n",
        "\n",
        "    return index\n",
        "\n",
        "indexPath=r\"index_path\"\n",
        "documentsPath=r\"documents_path\n",
        "indexgenerator(indexPath,documentsPath)\n"
      ],
      "metadata": {
        "id": "kJTAOVsQQKNj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}